{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cbe281d",
   "metadata": {},
   "source": [
    "# 사용자사전(단어뭉치) 만들기\n",
    "- https://somjang.tistory.com/entry/Google-Colab%EC%97%90%EC%84%9C-mecab-ko-dic-%EC%82%AC%EC%9A%A9%EC%9E%90-%EC%82%AC%EC%A0%84-%EC%B6%94%EA%B0%80%ED%95%98%EA%B8%B0\n",
    "- http://openuiz.blogspot.com/2018/12/mecab-ko-dic.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7ac0129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['대우,,,,NNP,*,F,대우,*,*,*,*,*\\n', '구글,,,,NNP,*,T,구글,*,*,*,*,*\\n']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab(dicpath=r\"C:/mecab/mecab-ko-dic\") \n",
    "\n",
    "# 종성여부 판단\n",
    "from jamo import h2j, j2hcj\n",
    "\n",
    "def get_jongsung_TF(sample_text):\n",
    "    sample_text_list = list(sample_text)\n",
    "    last_word = sample_text_list[-1]\n",
    "    last_word_jamo_list = list(j2hcj(h2j(last_word)))\n",
    "    last_jamo = last_word_jamo_list[-1]\n",
    "    jongsung_TF = \"T\"\n",
    "    if last_jamo in ['ㅏ', 'ㅑ', 'ㅓ', 'ㅕ', 'ㅗ', 'ㅛ', 'ㅜ', 'ㅠ', 'ㅡ', 'ㅣ', 'ㅘ', 'ㅚ', 'ㅙ', 'ㅝ', 'ㅞ', 'ㅢ', 'ㅐ,ㅔ', 'ㅟ', 'ㅖ', 'ㅒ']:\n",
    "        jongsung_TF = \"F\"\n",
    "    return jongsung_TF\n",
    "\n",
    "#기본 사용자사전 형식\n",
    "with open(\"C:/mecab/user-dic/nnp.csv\", 'r', encoding='utf-8') as f:\n",
    "    file_data = f.readlines()\n",
    "file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02e9e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가할 단어 리스트(config 파일 불러오기)\n",
    "from config import *\n",
    "_cfg = config\n",
    "\n",
    "user_dict = _cfg['USER_DICT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6655679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#종성여부 값 입력\n",
    "for word in user_dict:\n",
    "    jongsung_TF = get_jongsung_TF(word)\n",
    "    line = '{},,,,NNP,*,{},{},*,*,*,*,*\\n'.format(word, jongsung_TF, word)\n",
    "    file_data.append(line)\n",
    "    \n",
    "#사용자사전에 쓰기\n",
    "with open(\"C:/mecab/user-dic/nnp_new.csv\", 'w', encoding='utf-8') as f:\n",
    "    for line in list(set(file_data)): # 중복제거\n",
    "        f.write(line)\n",
    "#확인\n",
    "with open(\"C:/mecab/user-dic/nnp_new.csv\", 'r', encoding='utf-8') as f:\n",
    "    file_new = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e7c2242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# powershell이 실행 안 되므로 변수 reset\n",
    "%reset -f\n",
    "# PowerShell에서 값 적용 (https://joyhong.tistory.com/128)\n",
    "#https://stackoverflow.com/questions/73153170/run-powershell-as-administrator-from-python\n",
    "# See bottom section if you also want to get the exit code of \n",
    "# the elevated process.\n",
    "import subprocess\n",
    "p = subprocess.Popen(\n",
    "  [\n",
    "    \"powershell.exe\", \n",
    "    \"-noprofile\", \"-c\",\n",
    "    r\"\"\"\n",
    "    Start-Process -Verb RunAs -Wait powershell.exe -Args \"\n",
    "     -noprofile -c Set-ExecutionPolicy Unrestricted; cd c:\\mecab; .\\tools\\add-userdic-win.ps1;\n",
    "      \"\n",
    "    \"\"\"\n",
    "  ]#,shell=True #관리자권한\n",
    ")\n",
    "p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f797adf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 우선순위 조정\n",
    "df_ = pd.read_csv('C:/mecab/mecab-ko-dic/user-nnp_new.csv', header=None)\n",
    "df_[3] = list(map(lambda x: 0, df_[3]))\n",
    "df_.to_csv('C:/mecab/mecab-ko-dic/user-nnp_new.csv', header=None, index=False)\n",
    "\n",
    "# See bottom section if you also want to get the exit code of \n",
    "# the elevated process.\n",
    "import subprocess\n",
    "p = subprocess.Popen(\n",
    "  [\n",
    "    \"powershell.exe\", \n",
    "    \"-noprofile\", \"-c\",\n",
    "    r\"\"\"\n",
    "    Start-Process -Verb RunAs -Wait powershell.exe -Args \"\n",
    "     -noprofile -c Set-ExecutionPolicy Unrestricted; cd c:\\mecab; .\\tools\\compile-win.ps1;\n",
    "      \"\n",
    "    \"\"\"\n",
    "  ]#,shell=True #관리자권한\n",
    ")\n",
    "p.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5129d2ea",
   "metadata": {},
   "source": [
    "# 메모리를 초기화했기때문에 앞의 코드 재실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4a9664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab(dicpath=r\"C:/mecab/mecab-ko-dic\") \n",
    "\n",
    "# 종성여부 판단\n",
    "from jamo import h2j, j2hcj\n",
    "\n",
    "def get_jongsung_TF(sample_text):\n",
    "    sample_text_list = list(sample_text)\n",
    "    last_word = sample_text_list[-1]\n",
    "    last_word_jamo_list = list(j2hcj(h2j(last_word)))\n",
    "    last_jamo = last_word_jamo_list[-1]\n",
    "    jongsung_TF = \"T\"\n",
    "    if last_jamo in ['ㅏ', 'ㅑ', 'ㅓ', 'ㅕ', 'ㅗ', 'ㅛ', 'ㅜ', 'ㅠ', 'ㅡ', 'ㅣ', 'ㅘ', 'ㅚ', 'ㅙ', 'ㅝ', 'ㅞ', 'ㅢ', 'ㅐ,ㅔ', 'ㅟ', 'ㅖ', 'ㅒ']:\n",
    "        jongsung_TF = \"F\"\n",
    "    return jongsung_TF\n",
    "\n",
    "#기본 사용자사전 형식\n",
    "with open(\"C:/mecab/user-dic/nnp.csv\", 'r', encoding='utf-8') as f:\n",
    "    file_data = f.readlines()\n",
    "\n",
    "# 추가할 단어 리스트(config 파일 불러오기)\n",
    "from config import *\n",
    "_cfg = config\n",
    "user_dict = _cfg['USER_DICT']\n",
    "\n",
    "#종성여부 값 입력\n",
    "for word in user_dict:\n",
    "    jongsung_TF = get_jongsung_TF(word)\n",
    "    line = '{},,,,NNP,*,{},{},*,*,*,*,*\\n'.format(word, jongsung_TF, word)\n",
    "    file_data.append(line)\n",
    "    \n",
    "#사용자사전에 쓰기\n",
    "with open(\"C:/mecab/user-dic/nnp_new.csv\", 'w', encoding='utf-8') as f:\n",
    "    for line in list(set(file_data)): # 중복제거\n",
    "        f.write(line)\n",
    "#확인\n",
    "with open(\"C:/mecab/user-dic/nnp_new.csv\", 'r', encoding='utf-8') as f:\n",
    "    file_new = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a00fd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('갤럭시s105G', 'NNP')]\n",
      "[('갤럭시s23', 'NNP')]\n",
      "[('허위광고', 'NNP')]\n",
      "[('집중점검', 'NNP')]\n",
      "[('제조기', 'NNP')]\n",
      "[('부당광고', 'NNP')]\n",
      "[('식품첨가물', 'NNP')]\n",
      "[('생산실적', 'NNP')]\n",
      "[('대외직명', 'NNP')]\n",
      "[('거리두기', 'NNP')]\n",
      "[('누리집', 'NNP')]\n",
      "[('펜데믹', 'NNP')]\n",
      "[('식품안전정보', 'NNP')]\n",
      "[('식품안전정보포털', 'NNP')]\n",
      "[('식약처', 'NNP')]\n",
      "[('식품의약품안전처', 'NNP')]\n",
      "[('식품안전나라', 'NNP')]\n",
      "[('내손안', 'NNP')]\n",
      "[('공공데이터', 'NNP')]\n",
      "[('영업정지', 'NNP')]\n",
      "[('휴대전화', 'NNP')]\n",
      "[('이벤트', 'NNP')]\n",
      "[('코로나19', 'NNP')]\n",
      "[('영양성분', 'NNP')]\n",
      "[('건강기능식품', 'NNP')]\n",
      "[('수입식품', 'NNP')]\n",
      "[('공모전', 'NNP')]\n",
      "[('건강기능식품', 'NNP')]\n",
      "[('오메가3', 'NNP')]\n",
      "[('건강보조식품', 'NNP')]\n",
      "[('식중독', 'NNP')]\n",
      "[('수입식품정보마루', 'NNP')]\n",
      "[('CJ', 'NNP')]\n",
      "[('식품안전정보원', 'NNP')]\n",
      "[('금속', 'NNP')]\n",
      "[('이물질', 'NNP')]\n",
      "[('청정원', 'NNP')]\n",
      "[('와사비', 'NNP')]\n",
      "[('가금이력', 'NNP')]\n",
      "[('보완책', 'NNP')]\n",
      "[('친환경', 'NNP')]\n",
      "[('유기농', 'NNP')]\n",
      "[('회수판매중지', 'NNP')]\n",
      "[('판매중단', 'NNP')]\n",
      "[('회수조치', 'NNP')]\n",
      "[('안전관리', 'NNP')]\n",
      "[('어패류', 'NNP')]\n",
      "[('패류독소', 'NNP')]\n",
      "[('위반업체', 'NNP')]\n",
      "[('의료기기', 'NNP')]\n",
      "[('안전정보', 'NNP')]\n",
      "[('위생등급제', 'NNP')]\n",
      "[('기획전', 'NNP')]\n",
      "[('중국산', 'NNP')]\n",
      "[('일본산', 'NNP')]\n",
      "[('웹사이트', 'NNP')]\n",
      "[('영상공모전', 'NNP')]\n",
      "[('메타버스', 'NNP')]\n",
      "[('위생용품', 'NNP')]\n",
      "[('해외진출', 'NNP')]\n",
      "[('소비기한', 'NNP')]\n",
      "[('두부김치', 'NNP')]\n",
      "[('명절선물', 'NNP')]\n",
      "[('유통기한', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# 확인\n",
    "for word in user_dict:\n",
    "    print(mecab.pos(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1126395b",
   "metadata": {},
   "source": [
    "### 사용자사전 데이터기반생성\n",
    "- https://github.com/lovit/soynlp (새 기법)-[PyConKR2017] 노가다 없는 텍스트 분석을 위한 한국어 NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe68a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6226e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7e2ca71",
   "metadata": {},
   "source": [
    "# 텍스트 전처리 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f24d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "#path = os.listdir(\"C:/Users/user/Project/PJ2/News_Crawler/data/news_raw/\")\n",
    "#new_df = pd.read_csv(\"data/news/\"+path[0]) #최신파일\n",
    "df = pd.read_csv(\"C:/Users/user/Project/PJ1/News_Crawler/data/news_raw/title_식품안전나라_20160101_20221231.csv\")\n",
    "\n",
    "\"\"\" 필요 없는 문자 제거 \"\"\"\n",
    "def clean_text(row):\n",
    "    text = row['title']\n",
    "    pattern = r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"E-mail제거 : \" , text , \"\\n\")\n",
    "    pattern = r'(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"URL 제거 : \", text , \"\\n\")\n",
    "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"한글 자음 모음 제거 : \", text , \"\\n\")\n",
    "    pattern = '<[^>]*>'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"태그 제거 : \" , text , \"\\n\")\n",
    "    pattern = r'\\([^)]*\\)'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"괄호 및 괄호안 글자 제거 :  \" , text , \"\\n\")\n",
    "    pattern = r'[^\\w\\s]'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"특수기호 제거 : \", text , \"\\n\" )\n",
    "    pattern = r'[^\\w\\s]'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"필요없는 정보 제거 : \", text , \"\\n\" )\n",
    "    text = text.strip()\n",
    "    # print(\"양 끝 공백 제거 : \", text , \"\\n\" )\n",
    "    text = \" \".join(text.split())\n",
    "    # print(\"중간에 공백은 1개만 : \", text )\n",
    "    return text\n",
    "\n",
    "\n",
    "df['title_c'] = df.apply(clean_text, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15afd7c0",
   "metadata": {},
   "source": [
    "# 형태소 분석기\n",
    "- https://mr-doosun.tistory.com/22\n",
    "- https://passerby14.tistory.com/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4488d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\" 키워드 추출 from title \"\"\"\n",
    "##### Hannanum Class\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "##### Kkma Class\n",
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "##### Komoran Class\n",
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "##### Mecab Class\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab(dicpath=r\"C:/mecab/mecab-ko-dic\") \n",
    "##### Okt (Twitter) Class\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "\n",
    "#df = df[df['media'] != '코리아헤럴드']    # 코리아헤럴드는 영어 제목임\n",
    "df['keyword_hannanum'] = ''\n",
    "df['keyword_kkma'] = ''\n",
    "df['keyword_komoran'] = ''\n",
    "df['keyword_mecab'] = ''\n",
    "df['token_mecab'] = ''\n",
    "df['keyword_okt'] = ''\n",
    "\n",
    "for idx_line in tqdm(range(len(df))):\n",
    "    nouns_list_hannanum = hannanum.nouns(df['title_c'].loc[idx_line])\n",
    "    nouns_list_hannanum_c = [nouns for nouns in nouns_list_hannanum if len(nouns) > 1]    # 한글자는 이상한게 많아서 2글자 이상\n",
    "    df.loc[idx_line]['keyword_hannanum'] = nouns_list_hannanum_c #자체수정(원래는 set)\n",
    "\n",
    "    nouns_list_kkma = kkma.nouns(df['title_c'].loc[idx_line])\n",
    "    nouns_list_kkma_c = [nouns for nouns in nouns_list_kkma if len(nouns) > 1]    # 한글자는 이상한게 많아서 2글자 이상\n",
    "    df.loc[idx_line]['keyword_kkma'] = nouns_list_kkma_c #자체수정\n",
    "\n",
    "    nouns_list_komoran = komoran.nouns(df['title_c'].loc[idx_line])\n",
    "    nouns_list_komoran_c = [nouns for nouns in nouns_list_komoran if len(nouns) > 1]    # 한글자는 이상한게 많아서 2글자 이상\n",
    "    df.loc[idx_line]['keyword_komoran'] = nouns_list_komoran_c #자체수정\n",
    "\n",
    "    nouns_list_mecab = mecab.nouns(df['title_c'].loc[idx_line])\n",
    "    nouns_list_mecab_c = [nouns for nouns in nouns_list_mecab if len(nouns) > 1]    # 한글자는 이상한게 많아서 2글자 이상\n",
    "    df.loc[idx_line]['keyword_mecab'] = nouns_list_mecab_c #자체수정\n",
    "    \n",
    "    #형태소추출\n",
    "    token_list_mecab = mecab.morphs(df['title_c'].loc[idx_line])\n",
    "    token_list_mecab_c = [tokens for tokens in token_list_mecab if len(tokens) > 1]    # 한글자는 이상한게 많아서 2글자 이상\n",
    "    df.loc[idx_line]['token_mecab'] = token_list_mecab_c #자체수정\n",
    "\n",
    "    nouns_list_okt = okt.nouns(df['title_c'].loc[idx_line])\n",
    "    nouns_list_okt_c = [nouns for nouns in nouns_list_okt if len(nouns) > 1]    # 한글자는 이상한게 많아서 2글자 이상\n",
    "    df.loc[idx_line]['keyword_okt'] = nouns_list_okt_c #자체수정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa33681",
   "metadata": {},
   "source": [
    "# 불용어사전\n",
    "- https://github.com/Minku-Koo/Comment-Sentiment-Analysis/blob/main/python-code/top_word_graph.py (추가해야함)\n",
    "- url : https://www.ranks.nl/stopwords/korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4555f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = _cfg['STOP_WORD']\n",
    "\n",
    "for i in range(len(df['keyword_mecab'])):\n",
    "    df['keyword_mecab'][i] = [x for x in df['keyword_mecab'][i] if x not in stopword]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f58cc8c",
   "metadata": {},
   "source": [
    "# 동의어/유의어 사전\n",
    "- https://devocean.sk.com/blog/techBoardDetail.do?ID=164059"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0e143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_dict = _cfg['SYNONYM_DICT']\n",
    "\n",
    "apply_mapping = {word : k for k, v in synonym_dict.items() for word in v}\n",
    "result_list = pd.DataFrame([x for x in df['keyword_mecab']]).replace(apply_mapping, regex=True)\n",
    "\n",
    "for i in range(len(df['keyword_mecab'])):\n",
    "    df['keyword_mecab'][i] = sum(pd.DataFrame([x for x in df['keyword_mecab'][i]]).replace(apply_mapping, regex=True).values.T.tolist(), [])\n",
    "\n",
    "df['keyword_mecab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a435b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fada21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e7db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "118e8dff",
   "metadata": {},
   "source": [
    "# networkX\n",
    "- https://mons1220.tistory.com/241\n",
    "- https://velog.io/@hyunsuki/Personal-Project-%EB%85%BC%EB%AC%B8%EC%A0%9C%EB%AA%A9-%EB%B6%84%EC%84%9D%EC%9D%84-%ED%86%B5%ED%95%9C-%EC%84%A0%ED%96%89%EC%97%B0%EA%B5%AC-%EB%B6%84%EC%84%9DKeyword%EB%A5%BC-%EC%A4%91%EC%8B%AC%EC%9C%BC%EB%A1%9C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b4598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from __future__ import division\n",
    "from networkx.utils import random_state\n",
    "import sys\n",
    "import re\n",
    "import xlrd\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.utils import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import requests\n",
    "import urllib.request\n",
    "from selenium import webdriver\n",
    "import datetime\n",
    "import os\n",
    "import getpass\n",
    "from decorator import decorator\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "from math import log\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.manifold import TSNE\n",
    "from future.utils import iteritems\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49c20fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Edge list 작성 \"\"\"\n",
    "from collections import Counter\n",
    "\n",
    "edge_list = []\n",
    "for keywords in tqdm(df['keyword_mecab']):\n",
    "    #keywords = list(keywords_dict) #위에서 set으로 했으면 추가\n",
    "    num_keyword = len(keywords)\n",
    "    if num_keyword > 0:\n",
    "        for i in range(num_keyword-1):\n",
    "            for j in range(i+1, num_keyword):\n",
    "                edge_list += [tuple(sorted([keywords[i], keywords[j]]))]    # node 간 위해 sorted 사용\n",
    "edges = list(Counter(edge_list).items())\n",
    "\n",
    "\n",
    "\"\"\" networkx Graph 작성 \"\"\"\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph((x, y, {'weight': v}) for (x, y), v in edges)\n",
    "\n",
    "\"\"\" Community 추출 \"\"\"\n",
    "from community import community_louvain as lv\n",
    "partition = lv.best_partition(G)\n",
    "nx.set_node_attributes(G, partition, \"community\")   # graph G에 community 속성 추가\n",
    "\n",
    "\"\"\" Gephi file 작성 \"\"\"\n",
    "nx.write_gexf(G, '식품안전나라_뉴스.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c9a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#networkx Graph 시각화(랭킹없이 모든 엣지)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "font_path = \"C:/Windows/Fonts/malgunbd.ttf\" # 폰트 파일 위치\n",
    "font_name = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font_name)\n",
    "\n",
    "nx.Graph()\n",
    "plt.figure(figsize=(150,150))\n",
    "pos = nx.spring_layout(G, k=0.3) #k=0.0316\n",
    "nx.draw_networkx_nodes(G, pos,\n",
    "                node_shape = \"o\",\n",
    "                node_color='#BB78FF',\n",
    "                node_size=3000)\n",
    "nx.draw_networkx_edges(G, pos,\n",
    "                style='solid',\n",
    "                width=5,\n",
    "                alpha=0.3)\n",
    "nx.draw_networkx_labels(G, pos,\n",
    "                font_size=30,\n",
    "                font_family=font_name)\n",
    "plt.savefig(\"visualization/networkx_graph.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281409b",
   "metadata": {},
   "source": [
    "# LDA\n",
    "- https://lovit.github.io/nlp/2018/09/27/pyldavis_lda/#topic=0&lambda=1&term=\n",
    "- https://omicro03.medium.com/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-9%EC%9D%BC%EC%B0%A8-lda-%EC%8B%A4%EC%8A%B5-f6392e1ca958 (topic번호에 대한 설명)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일로 저장\n",
    "with open('data/sample_file.txt','w',encoding='UTF-8') as f:\n",
    "    for i in df['keyword_mecab']: #이미 전처리가 된 파일을 가져왔기때문에 추가적인 전처리 작업 없이 진행\n",
    "        f.write(' '.join(s for s in i)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9877d9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = 'data/sample_file.txt'\n",
    "#corpus_path = noun_list\n",
    "class Documents:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "    def __iter__(self):\n",
    "        with open(self.path, encoding='utf-8') as f:\n",
    "            for doc in f:\n",
    "                yield doc.strip().split()\n",
    "\n",
    "documents = Documents(corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b20782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(documents)\n",
    "print('dictionary size : %d' % len(dictionary)) # dictionary size : 37987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f648951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "min_count = 10\n",
    "word_counter = Counter((word for words in documents for word in words))\n",
    "removal_word_idxs = {\n",
    "    dictionary.token2id[word] for word, count in word_counter.items()\n",
    "    if count < min_count\n",
    "}\n",
    "\n",
    "dictionary.filter_tokens(removal_word_idxs)\n",
    "dictionary.compactify()\n",
    "print('dictionary size : %d' % len(dictionary)) # dictionary size : 10354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffa3816",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb8b75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter.most_common(10) #상위10개 순위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7768ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 워드클라우드\n",
    "# 사용한 Library\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "noun_list = word_counter.most_common(100)\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "#mask = Image.open('data/image/1.png')\n",
    "#mask = np.array(mask)\n",
    "font_path = 'C:\\\\Windows\\\\Fonts\\\\HMKMRHD.ttf'\n",
    "\n",
    "# wordcloud 만들기\n",
    "wc = WordCloud(font_path=font_path,\n",
    "                background_color=\"white\",\n",
    "                #mask = mask,\n",
    "                width=1000,\n",
    "                height=1000, \n",
    "                max_words=100,\n",
    "                max_font_size=300)\n",
    "    \n",
    "wc.generate_from_frequencies(dict(noun_list))\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.axis('off')\n",
    "plt.savefig(\"visualization/word_cloud\")\n",
    "plt.imshow(wc.generate_from_frequencies(dict(noun_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae1034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae41599",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, path, dictionary):\n",
    "        self.path = path\n",
    "        self.dictionary = dictionary\n",
    "        self.length = 0\n",
    "    def __iter__(self):\n",
    "        with open(self.path, encoding='utf-8') as f:\n",
    "            for doc in f:\n",
    "                yield self.dictionary.doc2bow(doc.split())\n",
    "    def __len__(self):\n",
    "        if self.length == 0:\n",
    "            with open(self.path, encoding='utf-8') as f:\n",
    "                for i, doc in enumerate(f):\n",
    "                    continue\n",
    "            self.length = i + 1\n",
    "        return self.length\n",
    "\n",
    "corpus = Corpus(corpus_path, dictionary)\n",
    "for i, doc in enumerate(corpus):\n",
    "    if i >= 5: break\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e785aab5",
   "metadata": {},
   "source": [
    "### 최적 토픽 수 산정\n",
    "- http://bigdata.emforce.co.kr/wp-content/uploads/%EC%97%A0%ED%8F%AC%EC%8A%A4-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%9E%A9_%ED%86%A0%ED%94%BD%EB%AA%A8%EB%8D%B8%EB%A7%81LDA%EB%B0%A9%EB%B2%95%EB%A1%A0-%EC%A0%95%EB%A6%AC.pdf\n",
    "- https://koreascience.kr/article/JAKO201708733756400.pdf\n",
    "- http://kj-ph.snu.ac.kr/xml/32134/32134.pdf\n",
    "- https://happy-obok.tistory.com/5#topic=4&lambda=1&term="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf3a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_values=[]\n",
    "for i in range(2,20):\n",
    "    lda_model = LdaModel(corpus, id2word=dictionary, num_topics=i)\n",
    "    coherence_model_lda = CoherenceModel(lda_model, texts=documents, dictionary=dictionary, topn=10)\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    coherence_values.append(coherence_lda)\n",
    "    \n",
    "x = range(2,20)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"number of topics\")\n",
    "plt.ylabel(\"coherence score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a617ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "perplexity_values=[]\n",
    "for i in range(2,20):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=i, id2word=dictionary)\n",
    "    perplexity_values.append(lda_model.log_perplexity(corpus))\n",
    "    \n",
    "x=range(2,20)\n",
    "plt.plot(x,perplexity_values)\n",
    "plt.xlabel(\"number of topics\")\n",
    "plt.ylabel(\"perplexity score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7859b4",
   "metadata": {},
   "source": [
    "### 토픽모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ff788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "lda_model = LdaModel(corpus, id2word=dictionary, num_topics=5) #토픽 개수\n",
    "with open('data/sample_file.pkl', 'wb') as f: #lda_model_path\n",
    "    pickle.dump(lda_model, f)\n",
    "#pd.read_pickle('data/news/sample_file.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4f6a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_term_prob(lda_model):\n",
    "    topic_term_freqs = lda_model.state.get_lambda()\n",
    "    topic_term_prob = topic_term_freqs / topic_term_freqs.sum(axis=1)[:, None]\n",
    "    return topic_term_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3196db21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model.alpha.shape) # (n_topics,)\n",
    "print(lda_model.alpha.sum()) # 1.0\n",
    "\n",
    "topic_term_prob = get_topic_term_prob(lda_model)\n",
    "print(topic_term_prob.shape)     # (n_topics, n_terms)\n",
    "print(topic_term_prob[0].sum())  # 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d14c42",
   "metadata": {},
   "source": [
    "##### LDAvis 는 본래 다섯 가지 정보를 입력받습니다.\n",
    "topic_term_dists # numpy.ndarray, shape = (n_topics, n_terms)\n",
    "doc_topic_dists  # numpy.ndarray, shape = (n_docs, n_topics)\n",
    "doc_lengths      # numpy.ndarray, shape = (n_docs,)\n",
    "vocab            # list of str, vocab list\n",
    "term_frequency   # numpy.ndarray, shape = (n_vocabs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe65e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "prepared_data = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(prepared_data)\n",
    "#pyLDAvis.save_html(prepared_data, 'data/news/LDA.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data[0] # 각 토픽이 차지하는 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1180ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.DataFrame()\n",
    "for i in range(len(prepared_data[0])):\n",
    "    raw_df = pd.concat([raw_df, pd.DataFrame(lda_model.show_topic(i, topn=8000))], axis=1)\n",
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e28b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850423a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "topic_vector = lda_model.expElogbeta\n",
    "y = PCA(n_components=2).fit_transform(topic_vector)\n",
    "\n",
    "print('{} -> {}'.format(topic_vector.shape, y.shape))\n",
    "# (n_topics, n_terms) -> (n_topics, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65db90a",
   "metadata": {},
   "source": [
    "# tf-idf 빈도분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc87d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "with open('data/sample_file.txt', 'r', encoding='utf-8') as f:\n",
    "    for doc in f:\n",
    "        corpus.append(doc.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f41c43",
   "metadata": {},
   "source": [
    "- https://wikidocs.net/31698"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506b577c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "print(tfidfv.transform(corpus).toarray())\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d0a5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfv.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06d7e41",
   "metadata": {},
   "source": [
    "- https://programmingjournal0813.tistory.com/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f4b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "sp_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "word2id = defaultdict(lambda : 0)\n",
    "for idx, feature in enumerate(vectorizer.get_feature_names_out()):\n",
    "    word2id[feature] = idx\n",
    "\n",
    "for i, sent in enumerate(corpus):\n",
    "    print('====== document[%d] ======' % i)\n",
    "    print( [ (token, sp_matrix[i, word2id[token]]) for token in sent.split() ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d138303",
   "metadata": {},
   "source": [
    "- https://hanawithdata.tistory.com/entry/%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%B6%84%EC%84%9D-%EB%8B%A8%EC%96%B4%EB%B9%88%EB%8F%84%EC%9D%98-%EA%B0%80%EC%A4%91%EC%B9%98-TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94173445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "tdm = tfidf.fit_transform(df['keyword_mecab'].apply(lambda x: ' '.join(x)))\n",
    "#tdm = tfidf.fit_transform(df['title_c']) #keyword_mecab으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9de1e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = pd.DataFrame({\n",
    "    '단어': tfidf.get_feature_names_out(),\n",
    "    '빈도': tdm.sum(axis=0).flat\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54efd26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count.sort_values('빈도', ascending=False).head(50)#.reset_index().drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1519597a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1423355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색날짜와 매칭 (날짜별 중요 키워드 반환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384d86a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74301049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254980d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2533b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18f17c00",
   "metadata": {},
   "source": [
    "# 기간별 빈도값, 시각화\n",
    "- https://github.com/mathpresso/Medium_material/blob/master/qanda_review/Qanda_review.ipynb\n",
    "- https://blog.mathpresso.com/python%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%BD%B4%EB%8B%A4-%EB%A6%AC%EB%B7%B0-%EB%B6%84%EC%84%9D-73b3f26e967c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6903bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_data(df, start_date, end_date):\n",
    "    query = df.copy()\n",
    "    query = query[query['date'].between(start_date, end_date)]\n",
    "    query['keyword_mecab'] = query['keyword_mecab'].apply(lambda text: ' '.join(x for x in text))\n",
    "    cnt = Counter((word for line in query['keyword_mecab'] for word in line.split()))\n",
    "    return query, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557ddc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1, counter1 = get_processed_data(df[['date','keyword_mecab']], '2016-01-01', '2016-12-31')\n",
    "query2, counter2 = get_processed_data(df[['date','keyword_mecab']], '2017-01-01', '2017-12-31')\n",
    "query3, counter3 = get_processed_data(df[['date','keyword_mecab']], '2018-01-01', '2018-12-31')\n",
    "query4, counter4 = get_processed_data(df[['date','keyword_mecab']], '2019-01-01', '2019-12-31')\n",
    "query5, counter5 = get_processed_data(df[['date','keyword_mecab']], '2020-01-01', '2020-12-31')\n",
    "query6, counter6 = get_processed_data(df[['date','keyword_mecab']], '2021-01-01', '2021-12-31')\n",
    "query7, counter7 = get_processed_data(df[['date','keyword_mecab']], '2022-01-01', '2022-11-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counter = pd.DataFrame([counter1, counter2, counter3, counter4, counter5, counter6, counter7]).T\n",
    "df_counter.columns = ['2016년', '2017년', '2018년', '2019년', '2020년', '2021년', '2022년']\n",
    "df_counter['total_sum'] = pd.DataFrame(df_counter.T.sum().sort_values(ascending=False))\n",
    "\n",
    "# 연도별 비교를 하려면 아래 옵션 주석처리\n",
    "df_counter = df_counter.fillna(0)\n",
    "df_counter = df_counter.apply(lambda x : x / np.sum(x))\n",
    "df_counter = df_counter.apply(lambda x : (x-min(x)) / (max(x)-min(x)))\n",
    "#df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c73f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,15))\n",
    "sns.heatmap(df_counter.sort_values(['total_sum'], ascending=False).iloc[:50], annot=True, linewidths=0.5, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f5176c",
   "metadata": {},
   "source": [
    "# Word2Vec 을 통해 키워드별 거리를 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed961d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "query, counter = get_processed_data(df[['date','keyword_mecab']], '2016-01-01', '2022-11-31')\n",
    "embedding_model = Word2Vec([line.split() for line in query['keyword_mecab']],vector_size=100, window=5, min_count=10, workers=4, sg=1, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1594df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8004bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly 이용해서 시각화하기\n",
    "import numpy as np\n",
    "import chart_studio\n",
    "\n",
    "#https://chart-studio.plotly.com/settings/api#/\n",
    "chart_studio.tools.set_credentials_file(username=_cfg['Plotly']['username'], api_key=_cfg['Plotly']['api_key'])\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7858f652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array([embedding_model.wv.get_vector(word) for word in embedding_model.wv.key_to_index.keys()])\n",
    "voca = list(embedding_model.wv.key_to_index.keys())\n",
    "X_embeded = TSNE(n_components=2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d165a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace2 = go.Scatter(\n",
    "    x=[x for x, y in X_embeded],\n",
    "    y=[y for x, y in X_embeded],\n",
    "    mode='markers+text',\n",
    "    name='Markers and Text',\n",
    "    text=voca,\n",
    "    textposition='bottom center'\n",
    ")\n",
    "\n",
    "data = [trace2]\n",
    "layout = go.Layout(\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='text-chart-basic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0def4689",
   "metadata": {},
   "source": [
    "# Word2vec 관련 키워드 (문장의 유사도인 코사인유사도-추천시스템-와 차이는??)\n",
    "- https://ssoonidev.tistory.com/93\n",
    "- https://github.com/ssooni/data_mining_practice/tree/master/regex_nlp_kko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ce0ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "def create_model(df, skip_gram=False):\n",
    "    tokens = df.copy()\n",
    "    tokens = tokens[tokens[\"keyword_mecab\"].apply(lambda x: 'http' not in x)]\n",
    "    sentence = tokens[\"keyword_mecab\"].tolist()\n",
    "\n",
    "    if skip_gram:\n",
    "        model = Word2Vec(sentence, min_count=10, epochs=20, vector_size=300, sg=1)\n",
    "    else:\n",
    "        model = Word2Vec(sentence, min_count=10, epochs=20, vector_size=300, sg=0)\n",
    "\n",
    "    model.init_sims(replace=True)\n",
    "    model.save(\"data/embedding.model\")\n",
    "\n",
    "\n",
    "def most_similar():\n",
    "    model = Word2Vec.load(\"data/embedding.model\")\n",
    "    print(\"식품안전나라와 관련된 키워드 : \", model.wv.most_similar(\"식품안전나라\"))\n",
    "    print(\"식약처와 관련된 키워드 : \", model.wv.most_similar(\"식약처\"))\n",
    "    #print(\"코로나와 관련된 키워드 : \", model.wv.most_similar(\"코로나\"))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_model(df)\n",
    "    most_similar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e66b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일단 필요없음 (불용어/사용자사전X)\n",
    "df['keyword'] = df['keyword_mecab'].apply(lambda text: ' '.join(x for x in text))\n",
    "\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "def create_model(df, skip_gram=False):\n",
    "    tokens = df.copy()\n",
    "    tokens = tokens[tokens[\"token_mecab\"].apply(lambda x: 'http' not in x)]\n",
    "    sentence = tokens[\"token_mecab\"].tolist()\n",
    "\n",
    "    if skip_gram:\n",
    "        model = Word2Vec(sentence, min_count=10, epochs=20, vector_size=300, sg=1)\n",
    "    else:\n",
    "        model = Word2Vec(sentence, min_count=10, epochs=20, vector_size=300, sg=0)\n",
    "\n",
    "    model.init_sims(replace=True)\n",
    "    model.save(\"data/embedding.model\")\n",
    "\n",
    "\n",
    "def most_similar():\n",
    "    model = Word2Vec.load(\"data/embedding.model\")\n",
    "    print(\"식품안전나라와 관련된 키워드 : \", model.wv.most_similar(\"식품안전나라\"))\n",
    "    print(\"식약처와 관련된 키워드 : \", model.wv.most_similar(\"식약처\"))\n",
    "    #print(\"코로나와 관련된 키워드 : \", model.wv.most_similar(\"코로나\"))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_model(df)\n",
    "    most_similar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171842d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff59c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279db74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_word = pd.read_excel('C:/Users/user/Project/PJ_foodinfo/data/Foodinfo/2. 식품안전나라 검색어 데이터/20220913_식품안전나라 검색어_Data_일검색10이상.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_fq = df_word.groupby('EXT_WORD').sum()['EXT_FRQ'].reset_index()\n",
    "df = df_word_fq[df_word_fq['EXT_FRQ'].values >= 4000]\n",
    "#df.sort_values('EXT_FRQ', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a21fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "#path = os.listdir(\"C:/Users/user/Project/PJ2/News_Crawler/data/news_raw/\")\n",
    "#new_df = pd.read_csv(\"data/news/\"+path[0]) #최신파일\n",
    "\n",
    "\"\"\" 필요 없는 문자 제거 \"\"\"\n",
    "def clean_text(row):\n",
    "    text = row['EXT_WORD']\n",
    "    pattern = '([ㄱ-ㅎㅏ-ㅣ]+)'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"한글 자음 모음 제거 : \", text , \"\\n\")\n",
    "    pattern = '<[^>]*>'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"태그 제거 : \" , text , \"\\n\")\n",
    "    pattern = r'\\([^)]*\\)'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"괄호 및 괄호안 글자 제거 :  \" , text , \"\\n\")\n",
    "    pattern = r'[^\\w\\s]'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"특수기호 제거 : \", text , \"\\n\" )\n",
    "    pattern = r'[^\\w\\s]'\n",
    "    text = re.sub(pattern=pattern, repl='', string=text)\n",
    "    # print(\"필요없는 정보 제거 : \", text , \"\\n\" )\n",
    "    text = text.strip()\n",
    "    # print(\"양 끝 공백 제거 : \", text , \"\\n\" )\n",
    "    text = \"\".join(text.split())\n",
    "    # print(\"중간 공백없애기 : \", text )\n",
    "    return text\n",
    "\n",
    "\n",
    "df['EXT_WORD'] = df.apply(clean_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803cde1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('EXT_FRQ', ascending=False)['EXT_WORD'][df.sort_values('EXT_FRQ', ascending=False)['EXT_WORD'].str.endswith('이란') == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f50ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_list = list(df['EXT_WORD'].unique())\n",
    "new_list.remove('')\n",
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d76e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea6fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aeca3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e6abea0",
   "metadata": {},
   "source": [
    "# SNA\n",
    "- https://foreverhappiness.tistory.com/38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- encoding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import operator\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\" Edge list 작성 \"\"\"\n",
    "from collections import Counter\n",
    "\n",
    "edge_list = []\n",
    "for keywords in tqdm(df['keyword_mecab']):\n",
    "    #keywords = list(keywords_dict) #위에서 set으로 했으면 추가\n",
    "    #keywords = keywords.split(' ') #리스트로 들어간 값이 아니라면 이걸로..\n",
    "    num_keyword = len(keywords)\n",
    "    if num_keyword > 0:\n",
    "        for i in range(num_keyword-1):\n",
    "            for j in range(i+1, num_keyword):\n",
    "                edge_list += [tuple(sorted([keywords[i], keywords[j]]))]    # node 간 위해 sorted 사용\n",
    "edges = list(Counter(edge_list).items())\n",
    "\n",
    "\"\"\" networkx Graph 작성 \"\"\"\n",
    "import networkx as nx\n",
    "G = nx.Graph((x, y, {'weight': v}) for (x, y), v in edges)\n",
    "G_ = G.copy()\n",
    "edges_ = [x for x in edges if int(x[1]) >= 5]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 중심성 척도 계산을 위한 Graph를 만든다\n",
    "    G_centrality = nx.Graph()\n",
    "\n",
    "    # 빈도수가 20000 이상인 단어쌍에 대해서만 edge(간선)을 표현한다.\n",
    "    for ind in range((len(edges_))):\n",
    "        G_centrality.add_edge(edges_[ind][0][0], edges_[ind][0][1], weight=edges_[ind][1])\n",
    "\n",
    "    dgr = nx.degree_centrality(G_centrality)        # 연결 중심성\n",
    "    btw = nx.betweenness_centrality(G_centrality)   # 매개 중심성\n",
    "    cls = nx.closeness_centrality(G_centrality)     # 근접 중심성\n",
    "    egv = nx.eigenvector_centrality(G_centrality)   # 고유벡터 중심성\n",
    "    pgr = nx.pagerank(G_centrality)                 # 페이지 랭크\n",
    "\n",
    "    # 중심성이 큰 순서대로 정렬한다.\n",
    "    sorted_dgr = sorted(dgr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_btw = sorted(btw.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_cls = sorted(cls.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_egv = sorted(egv.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_pgr = sorted(pgr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    # 단어 네트워크를 그려줄 Graph 선언\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # 페이지 랭크에 따라 두 노드 사이의 연관성을 결정한다. (단어쌍의 연관성)\n",
    "    # 연결 중심성으로 계산한 척도에 따라 노드의 크기가 결정된다. (단어의 등장 빈도수)\n",
    "    for i in range(len(sorted_pgr)):\n",
    "        G.add_node(sorted_pgr[i][0], nodesize=sorted_dgr[i][1])\n",
    "\n",
    "    for ind in range((len(edges_))):\n",
    "        G.add_weighted_edges_from([(edges_[ind][0][0], edges_[ind][0][1], edges_[ind][1])])\n",
    "        \n",
    "        \n",
    "    # 노드 크기 조정\n",
    "    sizes = [G.nodes[node]['nodesize'] * 20000 for node in G]\n",
    "\n",
    "    options = {\n",
    "        'edge_color': '#FFDEA2',\n",
    "        'width': 1,\n",
    "        'with_labels': True,\n",
    "        'font_weight': 'regular',\n",
    "    }\n",
    "\n",
    "    # 폰트 설정을 위한 font_manager import\n",
    "    import matplotlib.font_manager as fm\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # 폰트 설정\n",
    "    #fm._rebuild()# 1회에 한해 실행해준다. (폰트 새로고침, 여러번 해줘도 관계는 없다.)\n",
    "    font_fname = 'C:/Users/user/AppData/Local/Microsoft/Windows/Fonts/SDSamliphopangcheTTFBasic.ttf'      # 여기서 폰트는 C:/Windows/Fonts를 참고해도 좋다.\n",
    "    fontprop = fm.FontProperties(fname=font_fname, size=18).get_name()\n",
    "    plt.figure(figsize=(15,15))\n",
    "    nx.draw(G, node_size=sizes, pos=nx.spring_layout(G, k=3.5, iterations=100), **options, font_family=fontprop)  # font_family로 폰트 등록\n",
    "    ax = plt.gca()\n",
    "    ax.collections[0].set_edgecolor(\"#555555\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a882af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNA 단어 클릭시 해당 단어가 들어간 제목 보여주기, 관련된 연관어 보여주기 등.. 반응형으로!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
